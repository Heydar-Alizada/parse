import requests
from bs4 import BeautifulSoup
from typing import Dict, List
import random
import json
import os
import time

# –§–∞–π–ª –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π
SENT_ADS_FILE = 'sent_ads.json'
# –§–∞–π–ª –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∏–ª—å—Ç—Ä–æ–≤
FILTERS_FILE = 'filters.json'

def load_sent_ads() -> set:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π"""
    if os.path.exists(SENT_ADS_FILE):
        with open(SENT_ADS_FILE, 'r', encoding='utf-8') as f:
            return set(json.load(f))
    return set()

def save_sent_ads(ads: set) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ø–∏—Å–æ–∫ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π"""
    with open(SENT_ADS_FILE, 'w', encoding='utf-8') as f:
        json.dump(list(ads), f)

def load_filters() -> dict:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ñ–∏–ª—å—Ç—Ä—ã –∏–∑ —Ñ–∞–π–ª–∞"""
    if os.path.exists(FILTERS_FILE):
        with open(FILTERS_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {'title': [], 'location': []}

def save_filters(filters: dict) -> None:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ñ–∏–ª—å—Ç—Ä—ã –≤ —Ñ–∞–π–ª"""
    with open(FILTERS_FILE, 'w', encoding='utf-8') as f:
        json.dump(filters, f)

def apply_filters(title: str, location: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–∞–º"""
    filters = load_filters()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É
    for filter_text in filters['title']:
        if filter_text.lower() in title.lower():
            return False
            
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –ø–æ –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏—é
    for filter_text in filters['location']:
        if filter_text.lower() in location.lower():
            return False
            
    return True

def add_filter(filter_type: str, filter_text: str) -> None:
    """–î–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π —Ñ–∏–ª—å—Ç—Ä"""
    filters = load_filters()
    
    if filter_type in ['title', 'location']:
        if filter_text not in filters[filter_type]:
            filters[filter_type].append(filter_text)
            save_filters(filters)

def remove_filter(filter_type: str, filter_text: str) -> None:
    """–£–¥–∞–ª—è–µ—Ç —Ñ–∏–ª—å—Ç—Ä"""
    filters = load_filters()
    
    if filter_type in ['title', 'location']:
        if filter_text in filters[filter_type]:
            filters[filter_type].remove(filter_text)
            save_filters(filters)

def get_filters() -> dict:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â–∏–µ —Ñ–∏–ª—å—Ç—Ä—ã"""
    return load_filters()

def get_random_user_agent() -> str:
    """Return a random user agent string."""
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0) Gecko/20100101 Firefox/90.0',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36'
    ]
    return random.choice(user_agents)

def make_request(url: str, max_retries: int = 3) -> requests.Response:
    """Make a request with retries and random delays."""
    for attempt in range(max_retries):
        try:
            headers = {
                'User-Agent': get_random_user_agent(),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'az-AZ,az;q=0.9,en-US;q=0.8,en;q=0.7',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Cache-Control': 'no-cache'
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏
            if attempt > 0:
                delay = random.uniform(2, 5)
                print(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} –ø–æ—Å–ª–µ –∑–∞–¥–µ—Ä–∂–∫–∏ {delay:.1f} —Å–µ–∫")
                time.sleep(delay)
            
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            return response
            
        except requests.exceptions.RequestException as e:
            if attempt == max_retries - 1:  # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞
                raise
            print(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} –Ω–µ —É–¥–∞–ª–∞—Å—å: {str(e)}")
            continue

def parse_tap_az(url: str, user_filters: dict = None) -> list:
    """Parse tap.az website and return formatted results and photo URLs"""
    try:
        response = make_request(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        results = []
        items = soup.find_all('div', class_='products-i')
        
        print(f"–ù–∞–π–¥–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π tap.az: {len(items)}")
        
        for item in items[:10]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 10 –æ–±—ä—è–≤–ª–µ–Ω–∏–π
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                title_elem = item.find('div', class_='products-name')
                if not title_elem:
                    continue
                    
                title_text = title_elem.text.strip().lower()
                
                # –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫—É
                link = item.find('a', class_='products-link')
                if not link:
                    continue
                    
                href = link.get('href', '')
                if not href.startswith('http'):
                    href = 'https://tap.az' + href
                
                print(f"\n–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è tap.az: {title_text}")
                print(f"–°—Å—ã–ª–∫–∞: {href}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
                if user_filters:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É
                    if any(filter_text.lower() in title_text for filter_text in user_filters['title']):
                        print(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ –ø–æ —Ñ–∏–ª—å—Ç—Ä—É –∑–∞–≥–æ–ª–æ–≤–∫–∞: {title_text}")
                        continue
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –ø–æ –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏—é
                    location_elem = item.find('div', class_='products-location')
                    if location_elem:
                        location = location_elem.text.strip().lower()
                        if any(filter_text.lower() in location for filter_text in user_filters['location']):
                            print(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ –ø–æ —Ñ–∏–ª—å—Ç—Ä—É –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏—è: {location}")
                            continue
                
                # –ü–æ–ª—É—á–∞–µ–º —Ü–µ–Ω—É
                price_elem = item.find('div', class_='products-price')
                price = price_elem.text.strip() if price_elem else '–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞'
                
                # –ü–æ–ª—É—á–∞–µ–º —Ñ–æ—Ç–æ
                photo_elem = item.find('img')
                photo_url = None
                if photo_elem:
                    photo_url = photo_elem.get('data-src') or photo_elem.get('src')
                    if photo_url and not photo_url.startswith('http'):
                        photo_url = 'https:' + photo_url
                
                # –ò—â–µ–º –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ
                location_elem = item.find('div', class_='products-created')
                location_text = location_elem.text.strip() if location_elem else '–ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ –Ω–µ —É–∫–∞–∑–∞–Ω–æ'
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –ø–æ –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏—é
                if user_filters and any(f in location_text.lower() for f in user_filters['location']):
                    continue
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
                message = f"üè† {title_text}\n"
                message += f"üìç {location_text}\n"
                message += f"üí∞ {price}\n"
                message += f"üîó {href}"
                
                results.append({
                    'message': message,
                    'photo_url': photo_url,
                    'link': href  # –î–æ–±–∞–≤–ª—è–µ–º —Å—Å—ã–ª–∫—É –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                })
                
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è tap.az: {str(e)}")
                continue
                
        print(f"–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π tap.az: {len(results)}")
        return results
        
    except requests.RequestException as e:
        raise Exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ —Å–∞–π—Ç—É: {str(e)}")
    except Exception as e:
        raise Exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {str(e)}")

def parse_bina_az(url: str, user_filters: dict = None) -> list:
    """Parse bina.az website and return formatted results and photo URLs"""
    try:
        response = make_request(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        results = []
        
        # –ò—â–µ–º –≤—Å–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
        items = soup.find_all('div', class_='items-i') or soup.find_all('div', class_='items') or soup.find_all('div', class_='items-i-vip')
        
        print(f"–ù–∞–π–¥–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(items)}")
        
        for item in items[:10]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 10 –æ–±—ä—è–≤–ª–µ–Ω–∏–π
            try:
                # –ü–æ–ª—É—á–∞–µ–º —Ñ–æ—Ç–æ –∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–∑ –∞—Ç—Ä–∏–±—É—Ç–∞ alt
                photo_elem = item.find('img')
                if not photo_elem:
                    print("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ")
                    continue
                    
                full_title = photo_elem.get('alt', '').strip()
                if not full_title:
                    print("–ù–µ –Ω–∞–π–¥–µ–Ω –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤ –∞—Ç—Ä–∏–±—É—Ç–µ alt")
                    continue
                    
                print(f"\n–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è: {full_title}")
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏ —Ñ–æ—Ä–º–∏—Ä—É–µ–º —á–∏—Å—Ç—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
                location = "–ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ –Ω–µ —É–∫–∞–∑–∞–Ω–æ"
                title = full_title
                if " - " in full_title:
                    parts = full_title.split(" - ")
                    if len(parts) >= 2:
                        # –ë–µ—Ä–µ–º –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ –∏ –¥–µ–ª–∞–µ–º –ø–µ—Ä–≤—É—é –±—É–∫–≤—É –∑–∞–≥–ª–∞–≤–Ω–æ–π
                        location = parts[1].strip()
                        location = location[0].upper() + location[1:] if location else location
                        # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –±–µ–∑ –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏—è
                        title = parts[0].strip()
                        if len(parts) > 2:
                            title += " - " + parts[2].strip()
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
                if user_filters:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É
                    if any(filter_text.lower() in full_title.lower() for filter_text in user_filters['title']):
                        print(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ –ø–æ —Ñ–∏–ª—å—Ç—Ä—É –∑–∞–≥–æ–ª–æ–≤–∫–∞: {full_title}")
                        continue
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –ø–æ –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏—é
                    if any(filter_text.lower() in location.lower() for filter_text in user_filters['location']):
                        print(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ –ø–æ —Ñ–∏–ª—å—Ç—Ä—É –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏—è: {location}")
                        continue
                
                # –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫—É
                link = item.find('a', class_='item_link')
                if not link:
                    print("–ù–µ –Ω–∞–π–¥–µ–Ω–∞ —Å—Å—ã–ª–∫–∞ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ")
                    continue
                    
                href = link.get('href', '')
                if not href.startswith('http'):
                    href = 'https://bina.az' + href
                
                # –ü–æ–ª—É—á–∞–µ–º —Ü–µ–Ω—É
                price_elem = item.find('div', class_='items-price') or item.find('div', class_='price') or item.find('div', class_='items-price-vip')
                price = price_elem.text.strip() if price_elem else '–¶–µ–Ω–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞'
                
                # –ü–æ–ª—É—á–∞–µ–º URL —Ñ–æ—Ç–æ
                photo_url = photo_elem.get('data-src') or photo_elem.get('src')
                if photo_url and not photo_url.startswith('http'):
                    photo_url = 'https:' + photo_url
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
                message = f"üè† {title}\n"
                message += f"üìç {location}\n"
                message += f"üí∞ {price}\n"
                message += f"üîó {href}"
                
                results.append({
                    'message': message,
                    'photo_url': photo_url,
                    'link': href  # –î–æ–±–∞–≤–ª—è–µ–º —Å—Å—ã–ª–∫—É –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                })
                
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è bina.az: {str(e)}")
                continue
                
        print(f"–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(results)}")
        return results
        
    except requests.RequestException as e:
        raise Exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ —Å–∞–π—Ç—É: {str(e)}")
    except Exception as e:
        raise Exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {str(e)}")

def parse_website(url: str, user_filters: dict = None) -> list:
    """Parse website and return results."""
    if 'tap.az' in url:
        return parse_tap_az(url, user_filters)
    elif 'bina.az' in url:
        return parse_bina_az(url, user_filters)
    else:
        raise ValueError('–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Å–∞–π—Ç') 